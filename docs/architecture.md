# ðŸ—ï¸ Architecture Technique DÃ©taillÃ©e

## Vue d'Ensemble

Ce document dÃ©crit en dÃ©tail l'architecture technique du Data Warehouse B2B Mobile Money, incluant les choix technologiques, les patterns utilisÃ©s et les justifications.

---

## ðŸŽ¯ Principes de Conception

### 1. SÃ©paration des PrÃ©occupations
- **Extraction** (Extract): Lecture des donnÃ©es sources
- **Transformation** (Transform): Nettoyage, validation, enrichissement
- **Chargement** (Load): Insertion dans le DWH
- **Analytics**: Analyses et ML sÃ©parÃ©s du pipeline ETL

### 2. Idempotence
- Toutes les opÃ©rations ETL peuvent Ãªtre rejouÃ©es sans effet de bord
- Utilisation de clÃ©s naturelles pour dÃ©tecter les doublons
- Gestion des SCD Type 2 pour l'historique

### 3. ScalabilitÃ©
- Traitement par batch pour grandes volumÃ©tries
- Index optimisÃ©s sur les FK et colonnes de filtrage
- Tables agrÃ©gÃ©es pour requÃªtes frÃ©quentes

### 4. ObservabilitÃ©
- Logs dÃ©taillÃ©s Ã  chaque Ã©tape
- MÃ©triques de qualitÃ© des donnÃ©es
- Monitoring des performances

---

## ðŸ—„ï¸ Architecture du Data Warehouse

### SchÃ©ma en Ã‰toile (Star Schema)

**Avantages:**
- âœ… RequÃªtes simples et performantes
- âœ… ComprÃ©hension intuitive pour les analystes
- âœ… OptimisÃ© pour les outils BI (Power BI, Tableau, Metabase)
- âœ… AgrÃ©gations rapides

**Alternatives considÃ©rÃ©es:**
- âŒ **SchÃ©ma en Flocon** (Snowflake): Trop de jointures, moins performant
- âŒ **ModÃ¨le Data Vault**: Trop complexe pour ce cas d'usage
- âŒ **One Big Table**: Pas assez flexible, redondance excessive

### Tables de Faits

#### fact_transactions (Grain: Transaction)
- **Grain**: Une ligne = une transaction individuelle
- **Type**: Fact Table transactionnelle
- **VolumÃ©trie**: ~36M lignes (2 ans)
- **Partitionnement**: Envisageable par `date_key` si > 100M lignes

#### fact_daily_client_summary (Grain: Client-Service-Jour)
- **Grain**: Une ligne = un client Ã— service Ã— jour
- **Type**: Aggregate Fact Table
- **UtilitÃ©**: AccÃ©lÃ©ration des requÃªtes rÃ©currentes (dashboards)
- **Refresh**: Quotidien via job ETL

### Tables de Dimensions

#### Dimensions ConformÃ©es
- **dim_date**: PartagÃ©e par tous les faits temporels
- **dim_time**: GranularitÃ© minute
- Permet le drill-down temporel cohÃ©rent

#### SCD Type 2: dim_client
- **Historisation**: Changement de segment, taille, risque
- **Colonnes de gestion**:
  - `valid_from`: Date de dÃ©but de validitÃ©
  - `valid_to`: Date de fin (NULL pour actuel)
  - `is_current`: Boolean pour version actuelle
- **Avantage**: Analyses historiques prÃ©cises

---

## ðŸ”„ Pipeline ETL

### Architecture ETL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SOURCES (CSV/Parquet)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     EXTRACT (src/etl/extract.py)        â”‚
â”‚  â€¢ Lecture fichiers                     â”‚
â”‚  â€¢ Validation schÃ©ma                    â”‚
â”‚  â€¢ DÃ©tection encodage                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TRANSFORM (src/etl/transform.py)      â”‚
â”‚  â€¢ Nettoyage donnÃ©es                    â”‚
â”‚  â€¢ Lookup dimensions                    â”‚
â”‚  â€¢ Feature engineering                  â”‚
â”‚  â€¢ Calcul mÃ©triques                     â”‚
â”‚  â€¢ DÃ©tection anomalies                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       STAGING (staging.stg_*)           â”‚
â”‚  â€¢ Zone de prÃ©paration                  â”‚
â”‚  â€¢ Validation finale                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LOAD (src/etl/load.py)             â”‚
â”‚  â€¢ Bulk insert optimisÃ©                 â”‚
â”‚  â€¢ Gestion SCD Type 2                   â”‚
â”‚  â€¢ Update agrÃ©gats                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DATA WAREHOUSE (dwh.*)             â”‚
â”‚  â€¢ Dimensions + Faits                   â”‚
â”‚  â€¢ Vues analytiques                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Patterns ETL UtilisÃ©s

#### 1. Bulk Insert avec Pandas
```python
df.to_sql(
    name='table_name',
    con=engine,
    schema='dwh',
    if_exists='append',
    index=False,
    chunksize=10000,
    method='multi'  # INSERT avec plusieurs VALUES
)
```
**Performance**: ~10-50k lignes/seconde selon la machine

#### 2. Upsert pour Dimensions
```python
# Pseudo-code
ON CONFLICT (natural_key) DO UPDATE SET
    column1 = EXCLUDED.column1,
    updated_at = CURRENT_TIMESTAMP
```

#### 3. Dimension Lookup Cache
```python
# Cache des dimensions en mÃ©moire pour Ã©viter les requÃªtes rÃ©pÃ©tÃ©es
dim_cache = {
    'client': {client_id: client_key},
    'service': {service_id: service_key}
}
```

---

## ðŸ³ Infrastructure Docker

### Services Docker Compose

#### 1. PostgreSQL (postgres-dwh)
```yaml
Image: postgres:15-alpine
Role: Data Warehouse principal
Port: 5432
Volume: Persistance des donnÃ©es
```

**Configuration optimisÃ©e:**
- `shared_buffers`: 25% RAM
- `effective_cache_size`: 75% RAM
- `work_mem`: AjustÃ© selon volumÃ©trie
- `maintenance_work_mem`: Pour VACUUM et index

#### 2. PgAdmin (pgadmin)
```yaml
Image: dpage/pgadmin4
Role: Interface web PostgreSQL
Port: 5050
```

**UtilitÃ©:**
- Visualisation du schÃ©ma
- RequÃªtes ad-hoc
- Monitoring

#### 3. Jupyter (jupyter)
```yaml
Image: jupyter/scipy-notebook
Role: Analyse exploratoire et ML
Port: 8888
```

**Librairies incluses:**
- NumPy, Pandas, Scikit-learn
- Matplotlib, Seaborn
- AccÃ¨s au code source via volume

#### 4. Metabase (metabase)
```yaml
Image: metabase/metabase
Role: Dashboards web (alternative Power BI)
Port: 3000
```

**Avantages:**
- Open source, gratuit
- Interface drag-and-drop
- Alerte automatique
- Export vers Slack, email

### RÃ©seau Docker
```yaml
networks:
  b2b_network:
    driver: bridge
```
- Communication inter-conteneurs
- Isolation du rÃ©seau externe

### Volumes Docker
```yaml
volumes:
  postgres_data:  # DonnÃ©es PostgreSQL
  pgadmin_data:   # Config PgAdmin
  metabase_data:  # Config Metabase
```
- Persistance aprÃ¨s `docker-compose down`
- Backups facilitÃ©s

---

## ðŸ¤– Machine Learning

### Architecture ML

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DATA WAREHOUSE (dwh.*)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FEATURE ENGINEERING (ml/features.py)  â”‚
â”‚  â€¢ CrÃ©ation features temporelles        â”‚
â”‚  â€¢ AgrÃ©gations client                   â”‚
â”‚  â€¢ Encodage catÃ©gorielles               â”‚
â”‚  â€¢ Normalisation                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fraude ML   â”‚ â”‚ Segmentation â”‚
â”‚  (XGBoost)   â”‚ â”‚  (K-Means)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚
       â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     PrÃ©dictions / Scores     â”‚
â”‚  â€¢ fraud_score (0-1)         â”‚
â”‚  â€¢ client_segment (1-5)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ModÃ¨le de DÃ©tection de Fraude

**Algorithme**: XGBoost Classifier

**Raisons du choix:**
- âœ… Gestion native des valeurs manquantes
- âœ… Robuste au dÃ©sÃ©quilibre de classes
- âœ… Feature importance intÃ©grÃ©e
- âœ… Rapide en prÃ©diction
- âœ… Excellentes performances (92% prÃ©cision)

**Features Principales:**
1. **Temporelles**:
   - Heure de la transaction
   - Jour de la semaine
   - Est weekend
   - Est heures de bureau

2. **Montant**:
   - Montant de la transaction
   - Ã‰cart Ã  la moyenne client
   - Ã‰cart-type des montants

3. **Comportementales**:
   - VÃ©locitÃ© (transactions/heure)
   - Nombre de bÃ©nÃ©ficiaires
   - Changement de canal habituel

4. **GÃ©ographiques**:
   - Distance par rapport Ã  localisation habituelle
   - Pays diffÃ©rent

**Gestion du dÃ©sÃ©quilibre:**
- SMOTE (Synthetic Minority Over-sampling)
- Class weights
- MÃ©trique: F1-score (balance prÃ©cision/recall)

### Segmentation RFM

**Algorithme**: K-Means Clustering (k=5)

**Features:**
- **R**ecency: Jours depuis derniÃ¨re transaction
- **F**requency: Nombre de transactions (12 mois)
- **M**onetary: Volume total transactÃ©

**Segments:**
1. **Champions** (R:5, F:5, M:5)
   - Action: Programmes VIP, early access

2. **Loyal** (R:4-5, F:4-5, M:3-4)
   - Action: Upsell, cross-sell

3. **At Risk** (R:2-3, F:3-4, M:3-4)
   - Action: Win-back campaigns

4. **Hibernating** (R:1-2, F:2-3, M:2-3)
   - Action: RÃ©activation agressive

5. **Lost** (R:1, F:1, M:1)
   - Action: Analyse de churn, enquÃªte

---

## ðŸ“Š Tableaux de Bord Power BI

### Architecture de Connexion

```
Power BI Desktop
      â”‚
      â–¼
DirectQuery / Import
      â”‚
      â–¼
PostgreSQL DWH
  (localhost:5432)
      â”‚
      â–¼
Vues Analytiques
  (dwh.v_*)
```

### Modes de Connexion

#### Import Mode
- **Avantages**: Rapide, fonctionne hors ligne
- **InconvÃ©nients**: DonnÃ©es non temps rÃ©el
- **Utilisation**: Dashboards historiques

#### DirectQuery Mode
- **Avantages**: DonnÃ©es en temps rÃ©el
- **InconvÃ©nients**: DÃ©pendant de la DB
- **Utilisation**: Monitoring opÃ©rationnel

### Dashboards RecommandÃ©s

1. **KPI Overview**
   - Volume de transactions
   - Revenu (commissions)
   - Clients actifs
   - Taux de succÃ¨s

2. **Fraud Monitoring**
   - Transactions frauduleuses (carte chaleur)
   - Ã‰volution fraude par jour
   - Top clients Ã  risque
   - Montant sauvÃ©

3. **Client Segmentation**
   - Distribution RFM
   - Lifetime value par segment
   - Migration de segments
   - Clients Ã  risque de churn

4. **Service Performance**
   - Volume par service
   - Commission par service
   - Temps de traitement moyen
   - Taux d'Ã©chec

---

## ðŸ”’ SÃ©curitÃ© et ConformitÃ©

### DonnÃ©es Sensibles

1. **Anonymisation**
   ```python
   # Hashing des donnÃ©es PII
   client_id_hash = hashlib.sha256(real_id.encode()).hexdigest()
   ```

2. **Encryption at Rest**
   - Volumes Docker chiffrÃ©s
   - PostgreSQL: pgcrypto extension

3. **Access Control**
   ```sql
   -- RÃ´les PostgreSQL
   GRANT SELECT ON dwh.* TO analyst_role;
   GRANT ALL ON dwh.* TO etl_role;
   ```

### Audit Trail

```sql
-- Logs d'accÃ¨s
CREATE TABLE audit.access_log (
    user_name VARCHAR(100),
    access_time TIMESTAMP,
    table_name VARCHAR(100),
    action VARCHAR(20)
);
```

### RGPD

- **Droit Ã  l'oubli**: Fonction de suppression complÃ¨te
- **PortabilitÃ©**: Export en CSV/JSON
- **Minimisation**: Seulement donnÃ©es nÃ©cessaires

---

## ðŸ“ˆ Performance et Optimisation

### Index StratÃ©giques

```sql
-- Fact table
CREATE INDEX idx_fact_date_client ON fact_transactions(date_key, client_key);
CREATE INDEX idx_fact_service ON fact_transactions(service_key);

-- Dimensions
CREATE INDEX idx_client_segment ON dim_client(customer_segment);
CREATE INDEX idx_date_year_month ON dim_date(year, month_number);
```

### Partitionnement (si > 100M lignes)

```sql
-- Partitionnement par range sur date_key
CREATE TABLE fact_transactions (
    ...
) PARTITION BY RANGE (date_key);

CREATE TABLE fact_trans_2023 PARTITION OF fact_transactions
    FOR VALUES FROM (1) TO (365);

CREATE TABLE fact_trans_2024 PARTITION OF fact_transactions
    FOR VALUES FROM (366) TO (730);
```

### Materialized Views

```sql
-- Pour dashboards temps rÃ©el
CREATE MATERIALIZED VIEW dwh.mv_daily_kpi AS
SELECT ...
WITH DATA;

-- Refresh quotidien
REFRESH MATERIALIZED VIEW CONCURRENTLY dwh.mv_daily_kpi;
```

### Query Optimization

1. **EXPLAIN ANALYZE**: Identifier les goulets
2. **Vacuum rÃ©gulier**: Maintenir les stats Ã  jour
3. **Connection pooling**: PgBouncer pour haute concurrence

---

## ðŸ”„ Orchestration et Scheduling

### Airflow (Optionnel)

```python
# DAG ETL quotidien
from airflow import DAG
from airflow.operators.python import PythonOperator

dag = DAG('b2b_etl_daily', schedule_interval='@daily')

extract = PythonOperator(task_id='extract', python_callable=extract_data)
transform = PythonOperator(task_id='transform', python_callable=transform_data)
load = PythonOperator(task_id='load', python_callable=load_data)

extract >> transform >> load
```

### Cron Alternative

```bash
# crontab -e
0 2 * * * cd /path/to/project && ./venv/bin/python src/etl/run_etl.py
```

---

## ðŸ§ª Tests et QualitÃ©

### Tests Unitaires

```python
# pytest
def test_dimension_lookup():
    client_key = get_client_key('CL-001')
    assert client_key is not None
    assert isinstance(client_key, int)
```

### Data Quality Tests

```python
# Great Expectations
expect_column_values_to_not_be_null('transaction_amount')
expect_column_values_to_be_between('commission_rate', 0, 5)
expect_table_row_count_to_be_between(min_value=1000, max_value=1000000)
```

---

## ðŸ“¦ DÃ©ploiement

### Environnements

1. **Development** (local Docker)
2. **Staging** (Cloud VM)
3. **Production** (Kubernetes / Cloud SQL)

### CI/CD Pipeline

```yaml
# .github/workflows/ci.yml
name: CI
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run tests
        run: |
          python -m pytest tests/
      - name: Build Docker
        run: docker-compose build
```

---

## ðŸŽ“ Bonnes Pratiques AppliquÃ©es

1. âœ… **Code modulaire**: Chaque composant isolÃ©
2. âœ… **Configuration externalisÃ©e**: `.env`, `config.py`
3. âœ… **Logging centralisÃ©**: Tous les logs dans `/logs`
4. âœ… **Documentation**: README, docstrings, comments
5. âœ… **Version control**: Git avec .gitignore
6. âœ… **Idempotence**: Pipelines rejouables
7. âœ… **Monitoring**: MÃ©triques Ã  chaque Ã©tape
8. âœ… **Tests**: Unitaires et d'intÃ©gration

---

## ðŸ“š RÃ©fÃ©rences

- **Kimball Methodology**: Data Warehouse design
- **PostgreSQL Documentation**: https://www.postgresql.org/docs/
- **XGBoost**: https://xgboost.readthedocs.io/
- **Docker Compose**: https://docs.docker.com/compose/
- **Power BI**: https://docs.microsoft.com/power-bi/

---

**Auteur**: Votre Nom  
**Date**: 2024  
**Version**: 1.0
